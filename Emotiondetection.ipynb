{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5a4a55-4240-4b9b-b7a3-83a4b5d6caaf",
   "metadata": {},
   "source": [
    "# Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c085c5-4c50-4a27-8cf2-c33f5189bb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM Model...\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:22<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:23<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:33<00:00,  4.64it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:01<00:00, 12.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.00      0.00      0.00        29\n",
      "        Fear       0.54      1.00      0.70       150\n",
      "         Joy       0.00      0.00      0.00        68\n",
      "     Sadness       0.00      0.00      0.00        87\n",
      "    Surprise       0.00      0.00      0.00        85\n",
      "\n",
      "   micro avg       0.54      0.36      0.43       419\n",
      "   macro avg       0.11      0.20      0.14       419\n",
      "weighted avg       0.19      0.36      0.25       419\n",
      " samples avg       0.54      0.32      0.39       419\n",
      "\n",
      "Training RNN Model...\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:58<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:19<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:18<00:00,  8.41it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 26.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.00      0.00      0.00        29\n",
      "        Fear       0.54      1.00      0.70       150\n",
      "         Joy       0.00      0.00      0.00        68\n",
      "     Sadness       0.00      0.00      0.00        87\n",
      "    Surprise       0.00      0.00      0.00        85\n",
      "\n",
      "   micro avg       0.54      0.36      0.43       419\n",
      "   macro avg       0.11      0.20      0.14       419\n",
      "weighted avg       0.19      0.36      0.25       419\n",
      " samples avg       0.54      0.32      0.39       419\n",
      "\n",
      "Training CNN Model...\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:12<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:12<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 156/156 [00:12<00:00, 12.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:00<00:00, 42.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.00      0.00      0.00        29\n",
      "        Fear       0.65      0.74      0.69       150\n",
      "         Joy       0.42      0.12      0.18        68\n",
      "     Sadness       0.43      0.07      0.12        87\n",
      "    Surprise       0.68      0.42      0.52        85\n",
      "\n",
      "   micro avg       0.63      0.38      0.48       419\n",
      "   macro avg       0.44      0.27      0.30       419\n",
      "weighted avg       0.53      0.38      0.41       419\n",
      " samples avg       0.45      0.34      0.37       419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][selected_emotions].values.astype(float)  # Convert labels to float\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Define LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256, output_dim=len(selected_emotions)):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(30522, embedding_dim)  # Assuming vocab size of BERT tokenizer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        logits = self.fc(lstm_output[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "# Define RNN Model (Simple RNN)\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256, output_dim=len(selected_emotions)):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(30522, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        logits = self.fc(rnn_output[:, -1, :])\n",
    "        return logits\n",
    "\n",
    "# Define CNN Model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, embedding_dim=128, num_classes=len(selected_emotions)):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1d = nn.Conv1d(in_channels=embedding_dim,\n",
    "                                 out_channels=128,\n",
    "                                 kernel_size=3,\n",
    "                                 padding=1)  # Adjust padding as needed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids).permute(0, 2, 1)  # Change shape to (batch_size, embedding_dim, seq_length)\n",
    "        conv_output = self.relu(self.conv1d(embedded))\n",
    "        pooled_output = torch.max(conv_output, dim=2)[0]  # Max pooling over the time dimension\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize models and optimizers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "lstm_model = LSTMClassifier().to(device)\n",
    "rnn_model = RNNClassifier().to(device)\n",
    "cnn_model = CNNClassifier().to(device)\n",
    "\n",
    "optimizer_lstm = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "optimizer_cnn = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, optimizer):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids)\n",
    "        \n",
    "        loss_fn = nn.BCEWithLogitsLoss()  # Multi-label classification loss function\n",
    "        loss = loss_fn(outputs, labels)  # No need to apply sigmoid here\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate Function\n",
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    actuals_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].numpy()\n",
    "\n",
    "            outputs = model(input_ids).sigmoid()  # Apply sigmoid to get probabilities\n",
    "            \n",
    "            predictions_list.extend((outputs.cpu().numpy() > 0.5).astype(int))  # Convert probabilities to binary predictions\n",
    "            actuals_list.extend(labels)\n",
    "\n",
    "    return np.array(predictions_list), np.array(actuals_list)\n",
    "\n",
    "# Train and evaluate each model\n",
    "\n",
    "# Train LSTM Model\n",
    "print(\"Training LSTM Model...\")\n",
    "for epoch in range(3):  # Number of epochs can be adjusted\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_model(lstm_model, optimizer_lstm)\n",
    "\n",
    "lstm_predictions, lstm_actuals = evaluate_model(lstm_model)\n",
    "print(\"\\nLSTM Model Classification Report:\")\n",
    "print(classification_report(lstm_actuals, lstm_predictions, target_names=selected_emotions))\n",
    "\n",
    "# Train RNN Model\n",
    "print(\"Training RNN Model...\")\n",
    "for epoch in range(3):  # Number of epochs can be adjusted\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_model(rnn_model, optimizer_rnn)\n",
    "\n",
    "rnn_predictions, rnn_actuals = evaluate_model(rnn_model)\n",
    "print(\"\\nRNN Model Classification Report:\")\n",
    "print(classification_report(rnn_actuals, rnn_predictions, target_names=selected_emotions))\n",
    "\n",
    "# Train CNN Model\n",
    "print(\"Training CNN Model...\")\n",
    "for epoch in range(3):  # Number of epochs can be adjusted\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train_model(cnn_model, optimizer_cnn)\n",
    "\n",
    "cnn_predictions, cnn_actuals = evaluate_model(cnn_model)\n",
    "print(\"\\nCNN Model Classification Report:\")\n",
    "print(classification_report(cnn_actuals, cnn_predictions, target_names=selected_emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bbf4b03-36ce-4bcc-8558-3f9cbd0b76cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63d41e9ad0f407190dcd68a04e0c3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 156/156 [47:10<00:00, 18.14s/batch, loss=0.577]\n",
      "Epoch 2: 100%|██████████| 156/156 [49:06<00:00, 18.89s/batch, loss=0.46] \n",
      "Epoch 3: 100%|██████████| 156/156 [36:41<00:00, 14.11s/batch, loss=0.315]\n",
      "Evaluating: 100%|██████████| 18/18 [01:02<00:00,  3.49s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ALBERT Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       1.00      0.10      0.19        29\n",
      "        Fear       0.87      0.58      0.70       150\n",
      "         Joy       0.63      0.57      0.60        68\n",
      "     Sadness       0.91      0.37      0.52        87\n",
      "    Surprise       0.84      0.48      0.61        85\n",
      "\n",
      "   micro avg       0.81      0.48      0.60       419\n",
      "   macro avg       0.85      0.42      0.52       419\n",
      "weighted avg       0.84      0.48      0.59       419\n",
      " samples avg       0.54      0.45      0.48       419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./albert_emotion_model\\\\tokenizer_config.json',\n",
       " './albert_emotion_model\\\\special_tokens_map.json',\n",
       " './albert_emotion_model\\\\spiece.model',\n",
       " './albert_emotion_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training dataset\n",
    "train_file = 'public_data/train/track_a/eng.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "# Select relevant columns and convert emotions to multi-label binary format\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df[selected_emotions + ['text']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "df[selected_emotions] = df[selected_emotions].apply(lambda x: x > 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets (using the same data for both)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][selected_emotions].values.astype(float)  # Convert labels to float\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenizer for ALBERT\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize ALBERT Model for Multi-Label Classification\n",
    "model_albert = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=len(selected_emotions))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_albert.to(device)\n",
    "\n",
    "optimizer_albert = AdamW(model_albert.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop for ALBERT Model\n",
    "for epoch in range(3):\n",
    "    model_albert.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_albert.zero_grad()\n",
    "        \n",
    "        outputs = model_albert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_albert.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluate ALBERT Model on Test Set\n",
    "model_albert.eval()\n",
    "predictions_list = []\n",
    "actuals_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model_albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        \n",
    "        predictions_list.extend(preds)\n",
    "        actuals_list.extend(batch['labels'].numpy())\n",
    "\n",
    "# Convert predictions to binary (0 or 1) based on threshold (0.5)\n",
    "predictions_binary = (np.array(predictions_list) > 0.5).astype(int)\n",
    "\n",
    "# Print classification report for ALBERT Model\n",
    "print(\"\\nALBERT Model Classification Report:\")\n",
    "print(classification_report(actuals_list, predictions_binary, target_names=selected_emotions))\n",
    "\n",
    "# Save the trained ALBERT model and tokenizer in a specified folder\n",
    "model_save_path = './albert_emotion_model'\n",
    "model_albert.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cd7fa4-e239-47e0-ae3e-85be63321432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 156/156 [41:03<00:00, 15.79s/batch, loss=0.448]\n",
      "Epoch 2: 100%|██████████| 156/156 [40:56<00:00, 15.75s/batch, loss=0.302]\n",
      "Epoch 3: 100%|██████████| 156/156 [41:30<00:00, 15.96s/batch, loss=0.211]\n",
      "Evaluating: 100%|██████████| 18/18 [01:26<00:00,  4.83s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RoBERTa Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.60      0.62      0.61        29\n",
      "        Fear       0.72      0.89      0.80       150\n",
      "         Joy       0.77      0.53      0.63        68\n",
      "     Sadness       0.66      0.70      0.68        87\n",
      "    Surprise       0.79      0.59      0.68        85\n",
      "\n",
      "   micro avg       0.72      0.71      0.71       419\n",
      "   macro avg       0.71      0.67      0.68       419\n",
      "weighted avg       0.72      0.71      0.71       419\n",
      " samples avg       0.63      0.63      0.61       419\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./roberta_emotion_model\\\\tokenizer_config.json',\n",
       " './roberta_emotion_model\\\\special_tokens_map.json',\n",
       " './roberta_emotion_model\\\\vocab.json',\n",
       " './roberta_emotion_model\\\\merges.txt',\n",
       " './roberta_emotion_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training dataset\n",
    "train_file = 'public_data/train/track_a/eng.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "# Select relevant columns and convert emotions to multi-label binary format\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df[selected_emotions + ['text']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "df[selected_emotions] = df[selected_emotions].apply(lambda x: x > 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets (using the same data for both)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][selected_emotions].values.astype(float)  # Convert labels to float\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenizer for RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize RoBERTa Model for Multi-Label Classification\n",
    "model_roberta = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(selected_emotions))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_roberta.to(device)\n",
    "\n",
    "optimizer_roberta = AdamW(model_roberta.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop for RoBERTa Model\n",
    "for epoch in range(3):\n",
    "    model_roberta.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_roberta.zero_grad()\n",
    "        \n",
    "        outputs = model_roberta(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_roberta.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluate RoBERTa Model on Test Set\n",
    "model_roberta.eval()\n",
    "predictions_list = []\n",
    "actuals_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model_roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        \n",
    "        predictions_list.extend(preds)\n",
    "        actuals_list.extend(batch['labels'].numpy())\n",
    "\n",
    "# Convert predictions to binary (0 or 1) based on threshold (0.5)\n",
    "predictions_binary = (np.array(predictions_list) > 0.5).astype(int)\n",
    "\n",
    "# Print classification report for RoBERTa Model\n",
    "print(\"\\nRoBERTa Model Classification Report:\")\n",
    "print(classification_report(actuals_list, predictions_binary, target_names=selected_emotions))\n",
    "\n",
    "# Save the trained RoBERTa model and tokenizer in a specified folder\n",
    "model_save_path = './roberta_emotion_model'\n",
    "model_roberta.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "830dfaee-fc35-4127-b361-48ef89bae552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80223476889343f5b8809c3815873ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69501db7c444baad8436b8c7690b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92bab0b9d2f4764a4f7d2b30061054a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d5624e47e94149a802cf35eb37bf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74409e10e4cc4e87828310bd727363ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 156/156 [13:29<00:00,  5.19s/batch, loss=0.348]\n",
      "Epoch 2: 100%|██████████| 156/156 [13:02<00:00,  5.02s/batch, loss=0.448]\n",
      "Epoch 3: 100%|██████████| 156/156 [13:39<00:00,  5.25s/batch, loss=0.184]\n",
      "Evaluating: 100%|██████████| 18/18 [00:29<00:00,  1.61s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DistilBERT Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.80      0.28      0.41        29\n",
      "        Fear       0.73      0.88      0.80       150\n",
      "         Joy       0.73      0.44      0.55        68\n",
      "     Sadness       0.62      0.59      0.60        87\n",
      "    Surprise       0.70      0.62      0.66        85\n",
      "\n",
      "   micro avg       0.70      0.65      0.68       419\n",
      "   macro avg       0.72      0.56      0.60       419\n",
      "weighted avg       0.70      0.65      0.66       419\n",
      " samples avg       0.60      0.58      0.57       419\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert_emotion_model\\\\tokenizer_config.json',\n",
       " './distilbert_emotion_model\\\\special_tokens_map.json',\n",
       " './distilbert_emotion_model\\\\vocab.txt',\n",
       " './distilbert_emotion_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training dataset\n",
    "train_file = 'public_data/train/track_a/eng.csv'\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "# Select relevant columns and convert emotions to multi-label binary format\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df[selected_emotions + ['text']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "df[selected_emotions] = df[selected_emotions].apply(lambda x: x > 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets (using the same data for both)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][selected_emotions].values.astype(float)  # Convert labels to float\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize DistilBERT Model for Multi-Label Classification\n",
    "model_distilbert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(selected_emotions))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_distilbert.to(device)\n",
    "\n",
    "optimizer_distilbert = AdamW(model_distilbert.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop for DistilBERT Model\n",
    "for epoch in range(3):\n",
    "    model_distilbert.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_distilbert.zero_grad()\n",
    "        \n",
    "        outputs = model_distilbert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_distilbert.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluate DistilBERT Model on Test Set\n",
    "model_distilbert.eval()\n",
    "predictions_list = []\n",
    "actuals_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model_distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        \n",
    "        predictions_list.extend(preds)\n",
    "        actuals_list.extend(batch['labels'].numpy())\n",
    "\n",
    "# Convert predictions to binary (0 or 1) based on threshold (0.5)\n",
    "predictions_binary = (np.array(predictions_list) > 0.5).astype(int)\n",
    "\n",
    "# Print classification report for DistilBERT Model\n",
    "print(\"\\nDistilBERT Model Classification Report:\")\n",
    "print(classification_report(actuals_list, predictions_binary, target_names=selected_emotions))\n",
    "\n",
    "# Save the trained DistilBERT model and tokenizer in a specified folder\n",
    "model_save_path = './distilbert_emotion_model'\n",
    "model_distilbert.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b69b2743-5d17-45cb-9f4e-7bdb915d4116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 156/156 [45:19<00:00, 17.43s/batch, loss=0.349]\n",
      "Epoch 2: 100%|██████████| 156/156 [43:06<00:00, 16.58s/batch, loss=0.263]\n",
      "Epoch 3: 100%|██████████| 156/156 [52:58<00:00, 20.38s/batch, loss=0.145]\n",
      "Evaluating: 100%|██████████| 18/18 [01:37<00:00,  5.44s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.67      0.41      0.51        29\n",
      "        Fear       0.78      0.85      0.82       150\n",
      "         Joy       0.74      0.57      0.64        68\n",
      "     Sadness       0.73      0.53      0.61        87\n",
      "    Surprise       0.74      0.65      0.69        85\n",
      "\n",
      "   micro avg       0.75      0.67      0.71       419\n",
      "   macro avg       0.73      0.60      0.66       419\n",
      "weighted avg       0.75      0.67      0.70       419\n",
      " samples avg       0.63      0.60      0.60       419\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_file = 'public_data/train/track_a/eng.csv'\n",
    "test_file = 'public_data/test/track_a/eng_a.csv'\n",
    "\n",
    "df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "# Select the relevant columns\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df[selected_emotions + ['text']]\n",
    "\n",
    "# Convert emotions to multi-label binary format\n",
    "df[selected_emotions] = df[selected_emotions].apply(lambda x: x > 0).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][selected_emotions].values.astype(float)  # Convert labels to float\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = EmotionDataset(train_df, tokenizer, max_len=128)\n",
    "test_dataset = EmotionDataset(test_df, tokenizer, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# BERT Model\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(selected_emotions))\n",
    "model_bert = model_bert.to(device)\n",
    "\n",
    "optimizer_bert = AdamW(model_bert.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_loader) * 3\n",
    "scheduler_bert = get_linear_schedule_with_warmup(optimizer_bert, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training Loop for BERT Model\n",
    "for epoch in range(3):\n",
    "    model_bert.train()\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_bert.zero_grad()\n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer_bert.step()\n",
    "        scheduler_bert.step()\n",
    "        \n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "# Evaluate BERT Model on Test Set\n",
    "model_bert.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model_bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        actuals.extend(batch['labels'].numpy())\n",
    "\n",
    "# Convert predictions to binary (0 or 1) based on threshold (0.5)\n",
    "predictions_binary = (np.array(predictions) > 0.5).astype(int)\n",
    "\n",
    "# Print classification report for BERT Model\n",
    "print(\"\\nBERT Model Classification Report:\")\n",
    "print(classification_report(actuals, predictions_binary, target_names=selected_emotions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac704c-e4f3-41a7-b521-127e4dd4f031",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0078f149-4890-4d43-961c-6d76f3ae2343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I am so excited about the new movie release!\n",
      "Detected Emotions: Surprise\n",
      "\n",
      "Text: I feel anxious about the upcoming exam.\n",
      "Detected Emotions: Surprise\n",
      "\n",
      "Text: This betrayal makes me very angry and sad.\n",
      "Detected Emotions: Sadness, Anger, Fear\n",
      "\n",
      "Text: I am feeling quite down today but there was also a pleasant surprise.\n",
      "Detected Emotions: Joy\n",
      "\n",
      "Text: The surprise party was amazing and it made me very happy!\n",
      "Detected Emotions: Joy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Initialize the NLTK sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define the selected emotions\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "\n",
    "def classify_emotions(text):\n",
    "    try:\n",
    "        # Get the sentiment scores\n",
    "        scores = sia.polarity_scores(text)\n",
    "        \n",
    "        # Map sentiment scores to emotions\n",
    "        emotions = []\n",
    "        if scores['compound'] >= 0.5:\n",
    "            emotions.append('Joy')\n",
    "        elif scores['compound'] <= -0.5:\n",
    "            emotions.append('Sadness')\n",
    "            if scores['neg'] > 0.5:\n",
    "                emotions.append('Anger')\n",
    "        \n",
    "        if abs(scores['compound']) < 0.5 and scores['neu'] > 0.5:\n",
    "            emotions.append('Surprise')\n",
    "        \n",
    "        if scores['neg'] > 0.3 and 'Joy' not in emotions:\n",
    "            emotions.append('Fear')\n",
    "        \n",
    "        # If no emotions detected, return the dominant sentiment\n",
    "        if not emotions:\n",
    "            if scores['pos'] > scores['neg']:\n",
    "                return 'Positive'\n",
    "            elif scores['neg'] > scores['pos']:\n",
    "                return 'Negative'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "        \n",
    "        return ', '.join(emotions)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Example text inputs\n",
    "texts = [\n",
    "    \"I am so excited about the new movie release!\",\n",
    "    \"I feel anxious about the upcoming exam.\",\n",
    "    \"This betrayal makes me very angry and sad.\",\n",
    "    \"I am feeling quite down today but there was also a pleasant surprise.\",\n",
    "    \"The surprise party was amazing and it made me very happy!\"\n",
    "]\n",
    "\n",
    "# Classify emotions for each text\n",
    "for text in texts:\n",
    "    emotions = classify_emotions(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Detected Emotions: {emotions}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02be33a4-6554-411f-8377-afeb97c5c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: 'I am so happy with the outcome of the project!'\n",
      "LSTM Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "RNN Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "CNN Model Predictions:\n",
      "  Emotion 'Anger': Not Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "Sample Text: 'The news was really shocking and surprising.'\n",
      "LSTM Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "RNN Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "CNN Model Predictions:\n",
      "  Emotion 'Anger': Not Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "Sample Text: 'I am angry'\n",
      "LSTM Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "RNN Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "CNN Model Predictions:\n",
      "  Emotion 'Anger': Not Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "Sample Text: 'The movie was very sad and touching.'\n",
      "LSTM Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "RNN Model Predictions:\n",
      "  Emotion 'Anger': Present\n",
      "  Emotion 'Fear': Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n",
      "CNN Model Predictions:\n",
      "  Emotion 'Anger': Not Present\n",
      "  Emotion 'Fear': Not Present\n",
      "  Emotion 'Joy': Not Present\n",
      "  Emotion 'Sadness': Not Present\n",
      "  Emotion 'Surprise': Not Present\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#emotion labels\n",
    "selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "\n",
    "#function to preprocess text\n",
    "def preprocess_text(text, tokenizer, max_len=128):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    return encoding['input_ids'].flatten(), encoding['attention_mask'].flatten()\n",
    "\n",
    "#prediction function\n",
    "def predict(text):\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lstm_output = lstm_model(input_ids).sigmoid()\n",
    "        rnn_output = rnn_model(input_ids).sigmoid()\n",
    "        cnn_output = cnn_model(input_ids).sigmoid()\n",
    "\n",
    "    lstm_predicted_labels = (lstm_output.cpu().numpy() > 0.5).astype(int)[0]\n",
    "    rnn_predicted_labels = (rnn_output.cpu().numpy() > 0.5).astype(int)[0]\n",
    "    cnn_predicted_labels = (cnn_output.cpu().numpy() > 0.5).astype(int)[0]\n",
    "\n",
    "    return lstm_predicted_labels, rnn_predicted_labels, cnn_predicted_labels\n",
    "\n",
    "#sample texts\n",
    "sample_texts = [\n",
    "    \"I am so happy with the outcome of the project!\",\n",
    "    \"The news was really shocking and surprising.\",\n",
    "    \"I am angry\",\n",
    "    \"The movie was very sad and touching.\"\n",
    "]\n",
    "\n",
    "# format predictions\n",
    "def format_predictions(predictions):\n",
    "    return {\n",
    "        emotion: 'Present' if pred == 1 else 'Not Present'\n",
    "        for emotion, pred in zip(selected_emotions, predictions)\n",
    "    }\n",
    "\n",
    "# Print the formatted predictions for each sample text\n",
    "for sample_text in sample_texts:\n",
    "    print(f\"Sample Text: '{sample_text}'\")\n",
    "    \n",
    "    lstm_prediction, rnn_prediction, cnn_prediction = predict(sample_text)\n",
    "    \n",
    "    print(\"LSTM Model Predictions:\")\n",
    "    formatted_lstm_predictions = format_predictions(lstm_prediction)\n",
    "    for emotion, status in formatted_lstm_predictions.items():\n",
    "        print(f\"  Emotion '{emotion}': {status}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"RNN Model Predictions:\")\n",
    "    formatted_rnn_predictions = format_predictions(rnn_prediction)\n",
    "    for emotion, status in formatted_rnn_predictions.items():\n",
    "        print(f\"  Emotion '{emotion}': {status}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"CNN Model Predictions:\")\n",
    "    formatted_cnn_predictions = format_predictions(cnn_prediction)\n",
    "    for emotion, status in formatted_cnn_predictions.items():\n",
    "        print(f\"  Emotion '{emotion}': {status}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf74945-5037-448f-a7d7-de10246d17e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities for 'Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome. The constant pressure from work and personal expectations can be overwhelming, yet I strive to maintain a positive outlook. However, there are moments of doubt that creep in, especially when I reflect on past failures and the fear of repeating them. It's a bittersweet journey filled with highs and lows, but I know that every experience shapes who I am and I am joyful in my life.': [[0 0 1 1 0]]\n",
      "Predicted labels: {'Anger': 0, 'Fear': 0, 'Joy': 1, 'Sadness': 1, 'Surprise': 0}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_save_path='./bert_emotion_model'\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Define the device to use (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def predict(text):\n",
    "    encoding = loaded_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "    return (preds > 0.5).astype(int)\n",
    "\n",
    "complex_text = (\n",
    "    \"Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome. \"\n",
    "    \"The constant pressure from work and personal expectations can be overwhelming, yet I strive to maintain \"\n",
    "    \"a positive outlook. However, there are moments of doubt that creep in, especially when I reflect on past \"\n",
    "    \"failures and the fear of repeating them. It's a bittersweet journey filled with highs and lows, but I know \"\n",
    "    \"that every experience shapes who I am and I am joyful in my life.\"\n",
    ")\n",
    "\n",
    "predicted_probs = predict(complex_text)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "predicted_labels = (predicted_probs > 0.5).astype(int)[0]\n",
    "\n",
    "# Display detailed results\n",
    "emotion_labels = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "detailed_results = {emotion: predicted_labels[i] for i, emotion in enumerate(emotion_labels)}\n",
    "\n",
    "print(f\"Predicted probabilities for '{complex_text}': {predicted_probs}\")\n",
    "print(f\"Predicted labels: {detailed_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "318a39f2-5ac6-4216-99ad-3e90826c78fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome.'\n",
      "Detailed Prediction Output:\n",
      "Anger: Not Present\n",
      "Fear: Not Present\n",
      "Joy: Present\n",
      "Sadness: Not Present\n",
      "Surprise: Not Present\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "# Define the emotion classes (corresponding to your model's label indices)\n",
    "emotion_classes = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "\n",
    "# Load the saved ALBERT model and tokenizer\n",
    "model_save_path = './albert_emotion_model'\n",
    "loaded_model = AlbertForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = AlbertTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Define a prediction function using the loaded model and tokenizer\n",
    "def predict(text):\n",
    "    encoding = loaded_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "       preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    # Convert predictions to binary labels\n",
    "    binary_preds = (preds > 0.5).astype(int)\n",
    "    \n",
    "    # Map binary predictions to emotion classes\n",
    "    predicted_emotions = [emotion_classes[i] for i, pred in enumerate(binary_preds[0]) if pred == 1]\n",
    "    \n",
    "    return predicted_emotions\n",
    "\n",
    "# Example usage of prediction function with a complex text:\n",
    "complex_text = \"Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome.\"\n",
    "predicted_emotions = predict(complex_text)\n",
    "\n",
    "# Create a detailed output showing which emotions are present and which are not\n",
    "detailed_output = {emotion: (emotion in predicted_emotions) for emotion in emotion_classes}\n",
    "\n",
    "print(f\"Text: '{complex_text}'\")\n",
    "print(\"Detailed Prediction Output:\")\n",
    "for emotion, present in detailed_output.items():\n",
    "    print(f\"{emotion}: {'Present' if present else 'Not Present'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae281bd4-e5d3-4eb2-8ccd-829bcd9bc698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels for 'Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome but i am sad that i never succeeded.':\n",
      "Joy: Not Present\n",
      "Sadness: Not Present\n",
      "Anger: Present\n",
      "Fear: Present\n",
      "Surprise: Not Present\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the saved RoBERTa model and tokenizer (for future use)\n",
    "loaded_model = RobertaForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_model = loaded_model.to(device)  # Move the model to the device\n",
    "loaded_tokenizer = RobertaTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# List of emotions corresponding to the model's output labels\n",
    "emotions = ['joy', 'sadness', 'anger', 'fear', 'surprise', 'disgust']\n",
    "\n",
    "# Define a prediction function using the loaded model and tokenizer\n",
    "def predict(text):\n",
    "    encoding = loaded_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.sigmoid(outputs.logits).cpu().numpy()  # Get probabilities\n",
    "\n",
    "    return (preds > 0.5).astype(int)[0]  # Convert probabilities to binary (0 or 1)\n",
    "\n",
    "# Example usage of prediction function with a complex text:\n",
    "complex_text = \"Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome but i am sad that i never succeeded.\"\n",
    "predicted_labels = predict(complex_text)\n",
    "\n",
    "# Create a detailed output mapping the predicted labels to the emotions\n",
    "def detailed_emotion_output(predicted_labels):\n",
    "    emotion_results = {}\n",
    "    for i, label in enumerate(predicted_labels):\n",
    "        emotion_results[emotions[i]] = 'Present' if label == 1 else 'Not Present'\n",
    "    return emotion_results\n",
    "\n",
    "# Get detailed output\n",
    "detailed_output = detailed_emotion_output(predicted_labels)\n",
    "\n",
    "# Print detailed output\n",
    "print(f\"Predicted labels for '{complex_text}':\")\n",
    "for emotion, status in detailed_output.items():\n",
    "    print(f\"{emotion.capitalize()}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da743171-a8bb-410a-a9f6-b3e0f4609e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome.'\n",
      "Detailed Prediction Output:\n",
      "Anger: Not Present\n",
      "Fear: Not Present\n",
      "Joy: Present\n",
      "Sadness: Not Present\n",
      "Surprise: Not Present\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Define the emotion classes (corresponding to your model's label indices)\n",
    "emotion_classes = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "\n",
    "# Load the saved DistilBERT model and tokenizer\n",
    "model_save_path = './distilbert_emotion_model'\n",
    "loaded_model = DistilBertForSequenceClassification.from_pretrained(model_save_path)\n",
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained(model_save_path)\n",
    "\n",
    "# Define the device (CPU or GPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Define a prediction function using the loaded model and tokenizer\n",
    "def predict(text):\n",
    "    encoding = loaded_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "       outputs = loaded_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "       preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "\n",
    "    # Convert predictions to binary labels\n",
    "    binary_preds = (preds > 0.5).astype(int)\n",
    "    \n",
    "    # Map binary predictions to emotion classes\n",
    "    predicted_emotions = [emotion_classes[i] for i, pred in enumerate(binary_preds[0]) if pred == 1]\n",
    "    \n",
    "    return predicted_emotions\n",
    "\n",
    "# Example usage of prediction function with a complex text:\n",
    "complex_text = \"Despite facing numerous challenges in life, I find solace in my ability to adapt and overcome.\"\n",
    "predicted_emotions = predict(complex_text)\n",
    "\n",
    "# Create a detailed output showing which emotions are present and which are not\n",
    "detailed_output = {emotion: (emotion in predicted_emotions) for emotion in emotion_classes}\n",
    "\n",
    "print(f\"Text: '{complex_text}'\")\n",
    "print(\"Detailed Prediction Output:\")\n",
    "for emotion, present in detailed_output.items():\n",
    "    print(f\"{emotion}: {'Present' if present else 'Not Present'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5d5c3cb-28bb-4b2c-8e07-1d2b03acb815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n",
      "\n",
      "Text 1: The rollercoaster of emotions I experienced during the movie's climax left me breathless, tears streaming down my face even as I smiled at the bittersweet resolution.\n",
      "Predictions:\n",
      "Anger: 0.4656 (No)\n",
      "Fear: 0.5063 (Yes)\n",
      "Joy: 0.4453 (No)\n",
      "Sadness: 0.5369 (Yes)\n",
      "Surprise: 0.4720 (No)\n",
      "---\n",
      "\n",
      "Text 2: As I stood atop the mountain, gazing at the vast expanse before me, I felt a mix of exhilaration and trepidation, my heart racing with the thrill of accomplishment and the fear of the descent ahead.\n",
      "Predictions:\n",
      "Anger: 0.5751 (Yes)\n",
      "Fear: 0.5885 (Yes)\n",
      "Joy: 0.3489 (No)\n",
      "Sadness: 0.4909 (No)\n",
      "Surprise: 0.4803 (No)\n",
      "---\n",
      "\n",
      "Text 3: The unexpected news of my promotion filled me with a paradoxical blend of joy and anxiety, as I celebrated my success while grappling with the weight of new responsibilities.\n",
      "Predictions:\n",
      "Anger: 0.5834 (Yes)\n",
      "Fear: 0.6297 (Yes)\n",
      "Joy: 0.3331 (No)\n",
      "Sadness: 0.5186 (Yes)\n",
      "Surprise: 0.4679 (No)\n",
      "---\n",
      "\n",
      "Text 4: Watching the sunset over the ocean, I was overcome by a profound sense of peace tinged with a melancholic awareness of life's transient nature.\n",
      "Predictions:\n",
      "Anger: 0.5788 (Yes)\n",
      "Fear: 0.6307 (Yes)\n",
      "Joy: 0.3332 (No)\n",
      "Sadness: 0.5259 (Yes)\n",
      "Surprise: 0.4680 (No)\n",
      "---\n",
      "\n",
      "Text 5: The heated argument with my best friend left me feeling a tumultuous mix of anger, regret, and a desperate hope for reconciliation.\n",
      "Predictions:\n",
      "Anger: 0.4193 (No)\n",
      "Fear: 0.4688 (No)\n",
      "Joy: 0.4633 (No)\n",
      "Sadness: 0.4532 (No)\n",
      "Surprise: 0.4596 (No)\n",
      "---\n",
      "Testing completed.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AlbertTokenizer, AlbertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, num_labels, num_models):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.classifier = nn.Linear(num_labels * num_models, num_labels)\n",
    "        \n",
    "    def forward(self, model_outputs):\n",
    "        logits = self.classifier(model_outputs)\n",
    "        return logits\n",
    "\n",
    "def predict(model, tokenizer, texts, device, max_len=128):\n",
    "    model.eval()\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def ensemble_predict(ensemble_model, models, tokenizers, texts, device, max_len=128):\n",
    "    ensemble_model.eval()\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_outputs = []\n",
    "        for model, tokenizer in zip(models, tokenizers):\n",
    "            encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "            input_ids = encodings['input_ids'].to(device)\n",
    "            attention_mask = encodings['attention_mask'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            model_outputs.append(torch.sigmoid(logits))\n",
    "        \n",
    "        model_outputs = torch.cat(model_outputs, dim=1)\n",
    "        ensemble_logits = ensemble_model(model_outputs)\n",
    "        probs = torch.sigmoid(ensemble_logits).cpu().numpy()\n",
    "    \n",
    "    return probs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    selected_emotions = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load models and tokenizers\n",
    "    distilbert_path = './distilbert_emotion_model'\n",
    "    albert_path = './albert_emotion_model'\n",
    "    roberta_path = './roberta_emotion_model'\n",
    "    \n",
    "    distilbert_model = DistilBertForSequenceClassification.from_pretrained(distilbert_path).to(device)\n",
    "    albert_model = AlbertForSequenceClassification.from_pretrained(albert_path).to(device)\n",
    "    roberta_model = RobertaForSequenceClassification.from_pretrained(roberta_path).to(device)\n",
    "    \n",
    "    distilbert_tokenizer = DistilBertTokenizer.from_pretrained(distilbert_path)\n",
    "    albert_tokenizer = AlbertTokenizer.from_pretrained(albert_path)\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_path)\n",
    "    \n",
    "    ensemble_model = EnsembleModel(len(selected_emotions), 3).to(device)\n",
    "    \n",
    "    print(\"Models loaded successfully.\")\n",
    "    \n",
    "    # Test texts\n",
    "    test_texts = [\n",
    "        \"The rollercoaster of emotions I experienced during the movie's climax left me breathless, tears streaming down my face even as I smiled at the bittersweet resolution.\",\n",
    "        \"As I stood atop the mountain, gazing at the vast expanse before me, I felt a mix of exhilaration and trepidation, my heart racing with the thrill of accomplishment and the fear of the descent ahead.\",\n",
    "        \"The unexpected news of my promotion filled me with a paradoxical blend of joy and anxiety, as I celebrated my success while grappling with the weight of new responsibilities.\",\n",
    "        \"Watching the sunset over the ocean, I was overcome by a profound sense of peace tinged with a melancholic awareness of life's transient nature.\",\n",
    "        \"The heated argument with my best friend left me feeling a tumultuous mix of anger, regret, and a desperate hope for reconciliation.\"\n",
    "    ]\n",
    "    \n",
    "    # Make predictions\n",
    "    models = [distilbert_model, albert_model, roberta_model]\n",
    "    tokenizers = [distilbert_tokenizer, albert_tokenizer, roberta_tokenizer]\n",
    "    predictions = ensemble_predict(ensemble_model, models, tokenizers, test_texts, device)\n",
    "    \n",
    "    # Print results\n",
    "    for i, text in enumerate(test_texts):\n",
    "        print(f\"\\nText {i+1}: {text}\")\n",
    "        print(\"Predictions:\")\n",
    "        for emotion, prob in zip(selected_emotions, predictions[i]):\n",
    "            print(f\"{emotion}: {prob:.4f} ({'Yes' if prob > 0.5 else 'No'})\")\n",
    "        print(\"---\")\n",
    "\n",
    "print(\"Testing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874ff471-a317-4fff-baf7-b09e50f22179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 156/156 [19:22<00:00,  7.45s/it]\n",
      "100%|██████████| 156/156 [19:13<00:00,  7.39s/it]\n",
      "100%|██████████| 156/156 [19:01<00:00,  7.32s/it]\n",
      "100%|██████████| 156/156 [06:08<00:00,  2.36s/it]\n",
      "C:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 156/156 [31:39<00:00, 12.18s/it]\n",
      "100%|██████████| 156/156 [28:10<00:00, 10.83s/it]\n",
      "100%|██████████| 156/156 [27:54<00:00, 10.74s/it]\n",
      "100%|██████████| 156/156 [07:55<00:00,  3.05s/it]\n",
      "C:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 156/156 [23:21<00:00,  8.98s/it]\n",
      "100%|██████████| 156/156 [23:08<00:00,  8.90s/it]\n",
      "100%|██████████| 156/156 [23:15<00:00,  8.95s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    AlbertTokenizer, AlbertForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    AdamW\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.fillna('', inplace=True)  # Replace NaN values with an empty string\n",
    "    return df\n",
    "\n",
    "# Dataset Class for PyTorch\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        labels = self.df.iloc[index][['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Function to get predictions from a model\n",
    "def get_predictions(model, tokenizer, df):\n",
    "    dataset = EmotionDataset(df, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Load data\n",
    "df = load_data('public_data/train/track_a/eng.csv')\n",
    "\n",
    "# Initialize models and tokenizers\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5).to(device)\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "albert_model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=5).to(device)\n",
    "albert_tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=5).to(device)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Step 1: Train DistilBERT\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "distilbert_dataset = EmotionDataset(train_df, distilbert_tokenizer)\n",
    "distilbert_loader = DataLoader(distilbert_dataset, batch_size=16)\n",
    "\n",
    "optimizer_distilbert = AdamW(distilbert_model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    distilbert_model.train()\n",
    "    for batch in tqdm(distilbert_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_distilbert.zero_grad()\n",
    "        \n",
    "        outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_distilbert.step()\n",
    "\n",
    "# Get predictions from DistilBERT\n",
    "predictions_distilbert_train = get_predictions(distilbert_model, distilbert_tokenizer, train_df)\n",
    "\n",
    "# Check if the number of predictions matches the number of rows\n",
    "assert len(predictions_distilbert_train) == len(train_df), \\\n",
    "    f\"Number of predictions ({len(predictions_distilbert_train)}) does not match the number of rows ({len(train_df)})\"\n",
    "\n",
    "# Assign predictions to train_df_albert\n",
    "train_df_albert = train_df.copy()\n",
    "train_df_albert['DistilBERT_Predictions'] = predictions_distilbert_train.tolist()\n",
    "\n",
    "# Train ALBERT using DistilBERT outputs\n",
    "albert_dataset = EmotionDataset(train_df_albert, albert_tokenizer)\n",
    "albert_loader = DataLoader(albert_dataset, batch_size=16)\n",
    "\n",
    "optimizer_albert = AdamW(albert_model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    albert_model.train()\n",
    "    for batch in tqdm(albert_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_albert.zero_grad()\n",
    "        \n",
    "        outputs = albert_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_albert.step()\n",
    "\n",
    "# Get predictions from ALBERT\n",
    "predictions_albert_train = get_predictions(albert_model, albert_tokenizer, train_df)\n",
    "\n",
    "# Check if the number of predictions matches the number of rows\n",
    "assert len(predictions_albert_train) == len(train_df), \\\n",
    "    f\"Number of predictions ({len(predictions_albert_train)}) does not match the number of rows ({len(train_df)})\"\n",
    "\n",
    "# Prepare data for RoBERTa\n",
    "train_df_roberta = train_df.copy()\n",
    "train_df_roberta['DistilBERT_Predictions'] = predictions_distilbert_train.tolist()\n",
    "train_df_roberta['ALBERT_Predictions'] = predictions_albert_train.tolist()\n",
    "\n",
    "roberta_dataset = EmotionDataset(train_df_roberta, roberta_tokenizer)\n",
    "roberta_loader = DataLoader(roberta_dataset, batch_size=16)\n",
    "\n",
    "optimizer_roberta = AdamW(roberta_model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    roberta_model.train()\n",
    "    for batch in tqdm(roberta_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_roberta.zero_grad()\n",
    "        \n",
    "        outputs = roberta_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_roberta.step()\n",
    "\n",
    "# Combined dataset class for BERT\n",
    "class CombinedEmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        \n",
    "        # Combine previous model outputs as features for BERT input\n",
    "        distilbert_output = self.df.iloc[index]['DistilBERT_Predictions']\n",
    "        albert_output = self.df.iloc[index]['ALBERT_Predictions']\n",
    "        \n",
    "        labels = self.df.iloc[index][['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']].values.astype(float)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.float),\n",
    "            'distilbert_output': torch.tensor(distilbert_output, dtype=torch.float),\n",
    "            'albert_output': torch.tensor(albert_output, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "# Update the train_final_bert_model function\n",
    "def train_final_bert_model(model, tokenizer, df):\n",
    "    combined_dataset = CombinedEmotionDataset(df, tokenizer)\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    optimizer_final_bert = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(combined_loader, desc=f\"Training BERT Epoch {epoch+1}\", unit=\"batch\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer_final_bert.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_final_bert.step()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "# Initialize BERT model and tokenizer\n",
    "final_bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b36a7f9-4f1c-4f58-876c-363f5bf76c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models, tokenizers, and data have been saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def save_models_and_data(distilbert_model, albert_model, roberta_model, final_bert_model, \n",
    "                         distilbert_tokenizer, albert_tokenizer, roberta_tokenizer,\n",
    "                         train_df, test_df):\n",
    "    \n",
    "    # Create a directory to store all saved files\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(distilbert_model.state_dict(), 'saved_models/final_distilbert_model.pth')\n",
    "    torch.save(albert_model.state_dict(), 'saved_models/final_albert_model.pth')\n",
    "    torch.save(roberta_model.state_dict(), 'saved_models/final_roberta_model.pth')\n",
    "    torch.save(final_bert_model.state_dict(), 'saved_models/final_bert_model.pth')\n",
    "    \n",
    "    # Save tokenizers\n",
    "    distilbert_tokenizer.save_pretrained('saved_models/final_distilbert_tokenizer')\n",
    "    albert_tokenizer.save_pretrained('saved_models/final_albert_tokenizer')\n",
    "    roberta_tokenizer.save_pretrained('saved_models/final_roberta_tokenizer')\n",
    "    \n",
    "    # Save BERT tokenizer\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_tokenizer.save_pretrained('saved_models/final_bert_tokenizer')\n",
    "    \n",
    "    # Save DataFrames\n",
    "    train_df.to_pickle('saved_models/final_train_df.pkl')\n",
    "    test_df.to_pickle('saved_models/final_test_df.pkl')\n",
    "    \n",
    "    print(\"All models, tokenizers, and data have been saved successfully.\")\n",
    "\n",
    "# Assuming you have your models, tokenizers, and data ready, call the function like this:\n",
    "save_models_and_data(distilbert_model, albert_model, roberta_model, final_bert_model,\n",
    "                     distilbert_tokenizer, albert_tokenizer, roberta_tokenizer,\n",
    "                     train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c5d3fd0-c847-4446-a03c-5c995640d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_18564\\80481223.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  distilbert_model.load_state_dict(torch.load('saved_models/final_distilbert_model.pth', map_location=device))\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_18564\\80481223.py:58: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  albert_model.load_state_dict(torch.load('saved_models/final_albert_model.pth', map_location=device))\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_18564\\80481223.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  roberta_model.load_state_dict(torch.load('saved_models/final_roberta_model.pth', map_location=device))\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Vedant\\AppData\\Local\\Temp\\ipykernel_18564\\80481223.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_bert_model.load_state_dict(torch.load('saved_models/final_bert_model.pth', map_location=device))\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s]\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Predicting: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The intricate interplay between quantum mechanics and general relativity continues to puzzle physicists, as they strive to reconcile these fundamental theories into a unified framework of quantum gravity.\n",
      "Predictions:\n",
      "  Anger: 0.4566\n",
      "  Fear: 0.5543\n",
      "  Joy: 0.3810\n",
      "  Sadness: 0.5607\n",
      "  Surprise: 0.4202\n",
      "\n",
      "Text: In the realm of cognitive neuroscience, researchers are exploring the neural correlates of consciousness, attempting to unravel the mysteries of subjective experience and its relationship to brain activity.\n",
      "Predictions:\n",
      "  Anger: 0.4409\n",
      "  Fear: 0.5532\n",
      "  Joy: 0.3919\n",
      "  Sadness: 0.5261\n",
      "  Surprise: 0.4516\n",
      "\n",
      "Text: The advent of CRISPR-Cas9 gene editing technology has revolutionized molecular biology, offering unprecedented precision in genetic manipulation and raising profound ethical questions about the future of human evolution.\n",
      "Predictions:\n",
      "  Anger: 0.4519\n",
      "  Fear: 0.5577\n",
      "  Joy: 0.3888\n",
      "  Sadness: 0.5324\n",
      "  Surprise: 0.4399\n",
      "\n",
      "Text: Climate change presents a multifaceted challenge, intertwining environmental, economic, and social factors in a complex web of cause and effect that demands a coordinated global response.\n",
      "Predictions:\n",
      "  Anger: 0.4523\n",
      "  Fear: 0.5493\n",
      "  Joy: 0.3991\n",
      "  Sadness: 0.5444\n",
      "  Surprise: 0.4331\n",
      "\n",
      "Text: The emergence of artificial general intelligence poses both exciting possibilities and potential risks, as we grapple with the implications of creating machines that can match or surpass human cognitive abilities across a wide range of tasks.\n",
      "Predictions:\n",
      "  Anger: 0.4562\n",
      "  Fear: 0.5571\n",
      "  Joy: 0.3811\n",
      "  Sadness: 0.5449\n",
      "  Surprise: 0.4184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DistilBertForSequenceClassification, AlbertForSequenceClassification,\n",
    "    RobertaForSequenceClassification, BertForSequenceClassification,\n",
    "    DistilBertTokenizer, AlbertTokenizer, RobertaTokenizer, BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CombinedEmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, include_model_outputs=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.include_model_outputs = include_model_outputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.df.iloc[index]['text']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.include_model_outputs:\n",
    "            item['distilbert_output'] = torch.tensor(self.df.iloc[index]['DistilBERT_Predictions'], dtype=torch.float)\n",
    "            item['albert_output'] = torch.tensor(self.df.iloc[index]['ALBERT_Predictions'], dtype=torch.float)\n",
    "        \n",
    "        if 'Anger' in self.df.columns:\n",
    "            labels = self.df.iloc[index][['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']].values.astype(float)\n",
    "            item['labels'] = torch.tensor(labels, dtype=torch.float)\n",
    "        \n",
    "        return item\n",
    "\n",
    "def predict(model, texts, tokenizer, batch_size=16, include_model_outputs=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = CombinedEmotionDataset(texts, tokenizer, include_model_outputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.sigmoid(logits)\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Load models and data\n",
    "(distilbert_model, albert_model, roberta_model, final_bert_model,\n",
    " distilbert_tokenizer, albert_tokenizer, roberta_tokenizer, bert_tokenizer,\n",
    " train_df, test_df) = load_models_and_data()\n",
    "\n",
    "# Test with complex texts\n",
    "complex_texts = [\n",
    "    \"The intricate interplay between quantum mechanics and general relativity continues to puzzle physicists, as they strive to reconcile these fundamental theories into a unified framework of quantum gravity.\",\n",
    "    \"In the realm of cognitive neuroscience, researchers are exploring the neural correlates of consciousness, attempting to unravel the mysteries of subjective experience and its relationship to brain activity.\",\n",
    "    \"The advent of CRISPR-Cas9 gene editing technology has revolutionized molecular biology, offering unprecedented precision in genetic manipulation and raising profound ethical questions about the future of human evolution.\",\n",
    "    \"Climate change presents a multifaceted challenge, intertwining environmental, economic, and social factors in a complex web of cause and effect that demands a coordinated global response.\",\n",
    "    \"The emergence of artificial general intelligence poses both exciting possibilities and potential risks, as we grapple with the implications of creating machines that can match or surpass human cognitive abilities across a wide range of tasks.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame for the complex texts\n",
    "complex_df = pd.DataFrame({'text': complex_texts})\n",
    "\n",
    "# Get predictions from each model\n",
    "distilbert_preds = predict(distilbert_model, complex_df, distilbert_tokenizer)\n",
    "albert_preds = predict(albert_model, complex_df, albert_tokenizer)\n",
    "roberta_preds = predict(roberta_model, complex_df, roberta_tokenizer)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "complex_df['DistilBERT_Predictions'] = distilbert_preds.tolist()\n",
    "complex_df['ALBERT_Predictions'] = albert_preds.tolist()\n",
    "\n",
    "# Get final predictions from BERT\n",
    "final_preds = predict(final_bert_model, complex_df, bert_tokenizer, include_model_outputs=True)\n",
    "\n",
    "# Print results\n",
    "emotion_labels = ['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "for text, pred in zip(complex_texts, final_preds):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predictions:\")\n",
    "    for emotion, score in zip(emotion_labels, pred):\n",
    "        print(f\"  {emotion}: {score:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
